{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s8gre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import NeuronalNetwork as CNNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "class CNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(51136, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x,dim=1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_file(path: str):\n",
    "\n",
    "    dataset = []\n",
    "    walker = sorted(str(p) for p in Path(path).glob(f'*.wav'))\n",
    "\n",
    "    for i, file_path in enumerate(walker):\n",
    "        path, filename = os.path.split(file_path)    \n",
    "        # Load audio\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        dataset.append([waveform, sample_rate])\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNet()\n",
    "model.load_state_dict(torch.load('./data/model.pth'))\n",
    "model.eval()\n",
    "\n",
    "trainset_speechcommand = load_audio_file('./testaudio')\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(trainset_speechcommand, batch_size=1,\n",
    "                                            shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exists\n",
      "[[-20.323214  -24.938257  -22.680481  ...  -9.652863  -12.82978\n",
      "   -7.7300043]\n",
      " [-21.841885  -21.756824  -20.087286  ...  -8.05906   -10.274339\n",
      "   -8.517631 ]\n",
      " [-21.279036  -23.433155  -18.039644  ...  -7.9789057  -8.450219\n",
      "  -11.861316 ]\n",
      " ...\n",
      " [-26.869009  -25.95877   -25.062504  ... -24.650267  -28.011158\n",
      "  -30.295326 ]\n",
      " [-40.719624  -24.163355  -25.175873  ... -25.848898  -27.623299\n",
      "  -26.371065 ]\n",
      " [-25.931694  -23.238054  -26.589584  ... -23.138702  -25.016146\n",
      "  -24.734066 ]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "wrapped() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocessing\n\u001b[0;32m     14\u001b[0m scaler \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39mMinMaxScaler\n\u001b[1;32m---> 15\u001b[0m normalized \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(spectrogram_tensor[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mlog2()[\u001b[39m0\u001b[39;49m,:,:]\u001b[39m.\u001b[39;49mnumpy())\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(normalized)\n",
      "\u001b[1;31mTypeError\u001b[0m: wrapped() missing 1 required positional argument: 'X'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spectrogram_tensor = None\n",
    "directory = f'./data/testimage/order'\n",
    "if(os.path.isdir(directory)):\n",
    "    print(\"Data exists\")\n",
    "else:\n",
    "    os.makedirs(directory, mode=0o777, exist_ok=True)\n",
    "spectorgram_value = None\n",
    "for data in testloader: \n",
    "    waveform = data[0]\n",
    "    spectrogram_tensor = torchaudio.transforms.Spectrogram()(waveform)\n",
    "    fig = plt.figure()\n",
    "    fig = plt.figure()\n",
    "    plt.imsave(f'./data/testimage/order/test.png', spectrogram_tensor[0].log2()[0,:,:].numpy(), cmap='viridis')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "    root=\"./data/testimage/\",\n",
    "    transform=transforms.Compose([transforms.Resize((201,81)),\n",
    "                                  transforms.ToTensor()])\n",
    ")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=15,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"no\",\n",
    "    \"yes\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"no\"\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for batch, (X,Y) in enumerate(test_dataloader):\n",
    "    pred = model(X)\n",
    "    predicted = classes[pred[0].argmax(0)]\n",
    "    print(f'Predicted: \"{predicted}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7a041d1134705da44353d01ddd658c6575333281888fab0a4d55d2b3f61ed6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
